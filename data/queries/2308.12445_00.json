{
  "paper": "2308.12445.txt",
  "words": 10320,
  "extractions": {
    "description": "This paper proposes a novel self-healing approach for Deep Reinforcement Learning (DRL) systems called Dr. DRL, which integrates intentional forgetting mechanisms into Continual Learning (CL) to address issues like catastrophic forgetting and slow convergence in DRL systems. This approach selectively forgets minor behaviors in order to prioritize the DRL system's key problem-solving skills, thereby accelerating adaptation to environmental drifts and improving overall performance.",
    "title": {
      "value": "An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems",
      "confidence": 1.0,
      "justification": "The title is clearly mentioned at the beginning of the provided text.",
      "quote": "An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems"
    },
    "type": {
      "value": "Empirical Study",
      "confidence": 1.0,
      "justification": "The paper provides empirical evaluation results comparing Dr. DRL with vanilla Continual Learning across different environments and algorithms.",
      "quote": "To demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym environments with different drifting intensities."
    },
    "research_field": {
      "value": "Deep Learning",
      "confidence": 1.0,
      "justification": "The study focuses on the application and improvement of Deep Learning techniques to enhance DRL systems.",
      "quote": "Deep reinforcement learning (DRL), the blend of deep learning (DL) and reinforcement learning (RL), has shown promising achievements in recent years."
    },
    "sub_research_field": {
      "value": "Reinforcement Learning",
      "confidence": 1.0,
      "justification": "The focus is specifically on Deep Reinforcement Learning systems and how to enhance their adaptability and performance using a self-healing approach.",
      "quote": "...our intentional forgetting mechanism for an improved DRL healing."
    },
    "models": [
      {
        "name": {
          "value": "Deep Q-Learning (DQN)",
          "confidence": 1.0,
          "justification": "Deep Q-Learning (DQN) is mentioned as one of the well-established DRL algorithms evaluated in the paper.",
          "quote": "three well-established DRL algorithms, namely Deep Q-Learning (DQN), Soft Actor-Critic (SAC), and Proximal Policy Optimization (PPO)."
        },
        "role": "used",
        "type": "DRL",
        "mode": "inference"
      },
      {
        "name": {
          "value": "Soft Actor-Critic (SAC)",
          "confidence": 1.0,
          "justification": "Soft Actor-Critic (SAC) is mentioned as one of the well-established DRL algorithms evaluated in the paper.",
          "quote": "three well-established DRL algorithms, namely Deep Q-Learning (DQN), Soft Actor-Critic (SAC), and Proximal Policy Optimization (PPO)."
        },
        "role": "used",
        "type": "DRL",
        "mode": "inference"
      },
      {
        "name": {
          "value": "Proximal Policy Optimization (PPO)",
          "confidence": 1.0,
          "justification": "Proximal Policy Optimization (PPO) is mentioned as one of the well-established DRL algorithms evaluated in the paper.",
          "quote": "three well-established DRL algorithms, namely Deep Q-Learning (DQN), Soft Actor-Critic (SAC), and Proximal Policy Optimization (PPO)."
        },
        "role": "used",
        "type": "DRL",
        "mode": "inference"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CartPole",
          "confidence": 1.0,
          "justification": "CartPole is listed as one of the gym environments used for evaluation.",
          "quote": "three popular gym environments that we deliberately drift with varying parameters shifts, namely CartePole, MountainCar, and Acrobot."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "MountainCar",
          "confidence": 1.0,
          "justification": "MountainCar is listed as one of the gym environments used for evaluation.",
          "quote": "three popular gym environments that we deliberately drift with varying parameters shifts, namely CartePole, MountainCar, and Acrobot."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Acrobot",
          "confidence": 1.0,
          "justification": "Acrobot is listed as one of the gym environments used for evaluation.",
          "quote": "three popular gym environments that we deliberately drift with varying parameters shifts, namely CartePole, MountainCar, and Acrobot."
        },
        "role": "used"
      }
    ],
    "frameworks": [
      {
        "name": {
          "value": "TensorFlow",
          "confidence": 1.0,
          "justification": "TensorFlow is mentioned as the framework used to implement the proposed Dr. DRL approach.",
          "quote": "We implemented our approach as an open-source tool using Python 3.7 and it supports Tensorflow (version 2.4.4)."
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 911,
    "prompt_tokens": 16723,
    "total_tokens": 17634
  }
}